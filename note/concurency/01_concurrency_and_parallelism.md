# 並行性と並列性


## 目次

1. [プロセス](#プロセス)
1. [スレッド](#スレッド)
1. [並行性](#並行性)
1. [並列性](#並列性)
	1. [タスク並列性](#タスク並列性)
	1. [データ並列性](#データ並列性)
	1. [インストラクションレベル並列性](#インストラクションレベル並列性)
1. [計算速度](#計算速度)
	1. [アダムールの法則](#アダムールの法則)
1. [並列処理の重要性](#並列処理の重要性)
1. [並行処理の重要性](#並行処理の重要性)


## プロセス

**プロセス**は何らかの計算を行う計算実行体で、計算を完了するためにいくつかのステップを経た後、最終的に計算を停止する。プロセスには次の4つの状態からなる。

- **実行前状態**: 計算を実行する前の状態。実行状態へ遷移可能。
- **実行状態**: 計算を実行中の状態。待機状態か計算終了状態へ遷移可能。
- **待機状態**: 計算を一時停止中の状態。実行状態へ遷移可能。
- **終了状態**: 計算が終了した状態。

プロセスが待機状態に遷移する理由は主に3つある。

1つ目は、データの到着を待つためである。計算対象となるデータがない、もしくは到着していない場合は計算を実行することができないため、待機状態となる。

2つ目は、計算リソースの空きを待つためである。コンピュータは複数のプロセスを同時に実行することができ、他のプロセスがCPUを専有している間は計算を実行することができないため、待機状態となる。

3つ目は、自発的に待機状態となるためである。プログラム中でタイマなどを用いて処理を一時停止した場合などが該当する。


## スレッド

**スレッド**はプロセスと混合されがちであるが、プロセスがOSのカーネルから見た計算の実行単位であるのに対し、スレッドはプロセスに内包される。

OSが各プロセスに独立した仮想メモリ空間を割り当て、各スレッドは所属するプロセスの仮想メモリ空間とシステムリソース（ファイルディスクリプタなど）を共有している。Linuxの場合はスレッドは軽量なOSプロセスとして実装されているため、実装的にはほとんど差がないとも言える。


## 並行性

**並行性**とは、2つ以上のプロセスが同時に計算を進めている状態（実行状態もしくは待機状態）を指す言葉である。

同時に1つのプロセスしか扱えないOSのことを**シングルタスクOS**と呼び、並行処理可能なOSを**マルチタスクOS**と呼ぶ。シングルタスクOSとしては1980年代前半にマイクロソフト社が開発した**MS-DOS**（Windowsの前身）が有名である。現在広く普及しているOSである**Linux**、**BSD**や**Windows**はマルチタスクOSである。


## 並列性

**並列性**とは、同じ時刻で複数のプロセスが同時に実行状態にあることを意味する。また、コンピュータアーキテクチャ（ハードウェアレベル）から見た場合は次の3種類に分類される。

### タスク並列性

**タスク並列性**は、複数のタスク（プロセスあるいはスレッド）が同時に実行されていることを表す。

### データ並列性

**データ並列性**は、データを複数に分割して、分割したデータに対して並列に処理を行う方法である。**GPU（グラフィクスプロセッシングユニット）**を利用したベクトル演算などがデータ並列性を利用した例である。計算量の少ない問題では、スレッド生成や同期処理のオーバヘッドが大きいためデータ並列性を利用しても高速化には寄与しないため注意が必要である。

### インストラクションレベル並列性

**インストラクションレベル並列性**とは、インストラクション（CPUの命令語）レベルで並列化を行う手法である。主にハードウェアやコンパイラが暗黙的に行う並列化であり、プログラマがインストラクションレベル並列化を意識してプログラミングを行うことはほとんどない。

CPUは一定時間ごとに命令列を以下のようにいくつかのステップに分けて実行する。

- **命令読み込み**（IF: Instruction Fetch）: 次に実行する命令をメモリ上から読み込む
- **命令解釈**（ID: Instruction Decode）: 読み込んだ命令の解釈を行う
- **実行**（EX: Execution）: 実際に命令の実行を行う
- **メモリアクセス**（MEM: Memory Access）: メモリアクセスを行う（Read/Write）
- **書き込み**（WB: Write Back）: レジスタに演算結果を書き込む

この分割された小ステップのことを**パイプラインステージ**と呼び、分割する数のことを**パイプライン段数**と呼ぶ（上の例では5）。**パイプライン処理**ではこのように処理を分割することにより、実行中のステージ以外に空きができ、処理を並列化することができる。単純に計算するとパイプライン処理によりスループットがパイプライン段数と同じ数の倍率だけ向上するが、実際にはデータの依存関係などが原因で並列実行ができない**パイプラインハザード**が生じる。

- **構造ハザード**: ハードウェア的に並列実行できない命令を実行した場合に発生（同時メモリアクセスなど）
- **データハザード**: データの依存関係がある場合に発生（データ1の演算にデータ2の演算結果が必要な場合など）
- **制御ハザード**: 条件分岐がある場合に発生（命令1の結果によって読み込む命令が決まる場合など）


## 計算速度

**計算速度**は**応答速度**と**スループット**の2種類の尺度から考えることができる。

**応答速度**とは、計算が開始してから終了するまでの期間を示す。応答速度を表すために、**消費CPUクロック数**や**消費CPUインストラクション数**などが尺度として用いられているが、これらはすべて時間に還元可能である。

```math
応答速度 = \frac{消費CPUクロック数}{CPU動作クロック周波数} [\mathrm{s}]
応答速度 = \frac{消費CPUインストラクション数 \times CPI}{CPU動作クロック周波数} [\mathrm{s}]
```

**CPI**（Cycles Per Instruction）は1インストラクションあたり平均何CPUサイクルを消費するかを示す値である。

**スループット**とは、単位時間あたりに実行可能な計算量を表すものであり、その単位にはMIPS（Million Instructions Per Second）やFLOPS（Loating point number Operations Per Second）などが尺度として用いられる。

### アダムールの法則

並列化することでどの程度応答速度が向上するかは、並列化可能な処理の割合と並列化に伴うオーバヘッドによって決まる。並列化による応答速度の向上率は、**アダムールの法則**により以下の式で得られる。

```math
\frac{1}{(1 - P) + \frac{P}{N}}

（オーバヘッドを考慮した場合）
\frac{1}{H + (1 - P) + \frac{P}{N}}
```

`P` は全体のプログラム中において並列化可能な処理が占める割合、 `N` は並列化の数、 `H` はオーバヘッドの応答速度と逐次実行したときの応答速度の比である。


## 並列処理の重要性

ソフトウェア開発において主に意識しなければならないタスク並列性を意識しなければならない理由としては、ハードウェア（半導体技術）の限界があげられる。

CPUなどのチップはシリコンウェハという薄い円盤上のシリコン上に印刷するような形（**ダイ**）で製造される。半導体素子の微細化により面積あたりにより多くのダイを作成できるようになり、ダイの製造単価が下がる。また、半導体で作成されたトランジスタのオンオフを高速化でき、CPUのクロック数を上げることができるという利点もある。しかし、動作周波数を上げると消費電力が多くなり発熱も多くなり、半導体素子の劣化が激しくなり、動作が不安定になってしまう。

半導体素子の微細化は、半導体トランジスタ内における**リーク電流**などの問題もあり限界があるため、ソフトウェアレイヤが並列化を意識しなければいけない流れとなってきている。


## 並行処理の重要性

並行処理は計算リソースの効率的な活用と公平性、利便性といった面で重要とされる。並行処理を活用することにより、あるプロセスがIO待ちなどの待機状態にあるときに計算リソースを利用することができる。また、並行処理可能なOSにおいては、音楽を聴きながらインターネットをブラウジングできるといった利点もある。
